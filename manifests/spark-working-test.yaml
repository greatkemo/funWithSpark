apiVersion: batch/v1
kind: Job
metadata:
  name: spark-working-test
spec:
  template:
    spec:
      serviceAccountName: spark
      restartPolicy: Never
      containers:
      - name: spark-working
        image: docker.io/bitnami/spark:3.5.1-debian-12-r0
        command: ["/bin/bash"]
        args:
        - -c
        - |
          echo "=== Working Spark Test ==="
          
          # Set proper environment variables to fix Ivy issue
          export SPARK_USER=spark
          export HOME=/tmp/spark-home
          mkdir -p $HOME/.ivy2/local
          
          # Create a simple Java program instead of using Spark Shell
          cat > /tmp/SparkPiTest.java << 'EOF'
          import org.apache.spark.SparkConf;
          import org.apache.spark.api.java.JavaSparkContext;
          import org.apache.spark.api.java.JavaRDD;
          import java.util.Arrays;
          import java.util.List;
          
          public class SparkPiTest {
              public static void main(String[] args) {
                  SparkConf conf = new SparkConf().setAppName("SparkPiTest").setMaster("local[2]");
                  JavaSparkContext sc = new JavaSparkContext(conf);
                  
                  int n = 100000;
                  List<Integer> l = Arrays.asList(1, 2, 3, 4, 5);
                  JavaRDD<Integer> rdd = sc.parallelize(l);
                  
                  System.out.println("=== SPARK TEST RESULTS ===");
                  System.out.println("RDD count: " + rdd.count());
                  System.out.println("RDD first element: " + rdd.first());
                  System.out.println("=== SUCCESS: Spark is working! ===");
                  
                  sc.close();
              }
          }
          EOF
          
          # Try a simpler approach - just test Spark submit with built-in examples
          echo "Testing Spark with built-in Pi example..."
          /opt/bitnami/spark/bin/spark-submit \
            --class org.apache.spark.examples.SparkPi \
            --master local[2] \
            --deploy-mode client \
            --conf spark.sql.warehouse.dir=/tmp/spark-warehouse \
            --conf spark.driver.host=localhost \
            --conf spark.driver.bindAddress=0.0.0.0 \
            --conf spark.sql.adaptive.enabled=false \
            --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
            /opt/bitnami/spark/examples/jars/spark-examples_2.12-3.5.1.jar 10 2>&1 | grep -E "(Pi is|ERROR|Exception)" || echo "Pi calculation completed"
            
          echo "=== Test completed ==="
        env:
        - name: SPARK_USER
          value: "spark"
        - name: SPARK_HOME
          value: "/opt/bitnami/spark"
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
